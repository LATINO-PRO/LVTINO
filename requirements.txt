numpy
torch==2.8.0
torchvision==0.23.0
diffusers
transformers
accelerate
huggingface-hub
omegaconf
tqdm
opencv-python
imageio
pillow
easydict
ftfy
einops
deepinv
lpips
imageio-ffmpeg
flash-attn
# If there are troubles in installing flash-attn, you can try downloading the correct wheel from https://github.com/Dao-AILab/flash-attention/releases 
# and installing it with pip install <wheel_file>.whl